{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large number of Monte Carlo methods exist for generating samples or estimating quantities of interest from a chosen distribution. Such methods are usually used when these distributions are complex and standard analytical methods for accomplishing these goals are unavailable. Monte Carlo methods are defined by the use of pseudo-random numbers, hence their nominal connection to the famous [Monte Carlo casino](https://en.wikipedia.org/wiki/Monte_Carlo_Casino).\n",
    "\n",
    "Here, we briefly explore one technique called importance sampling. This will give you some sense for how Monte Carlo methods work generally, but also why this fairly simple approach is not efficient for complex problems, such as arise in phylogenetics. As we discuss this and other Monte Carlo approaches, we will leave formal proofs of their accuracy and more verbose descriptions of the relevant background to the textbooks. Instead, we will focus on providing an intuition for how they work.\n",
    "\n",
    "To begin, we're just going to import some modules and classes that we'll use later to calculate probabilities and create plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the binomial and uniform classes from the stats module of the scipy library\n",
    "from scipy.stats import binom, uniform\n",
    "\n",
    "# Importing pseudo-random number generators for uniform and Gaussian distributions\n",
    "from random import random, gauss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did with the Introduction to Likelihood, we're going to use a coin-flipping binomial example. Bear in mind, however, that all of the estimation machinery is general to any sort of distribution in which we may be interested. In fact, feel free to make a copy of this notebook, then substitute a different type of data. You'll just need to change the function calls for calculating probabilities and the range of parameter values you're considering. I will attempt to highlight the areas that would need to be changed with comments in the code.\n",
    "\n",
    "Here, we're starting with 5 coin flips and using the same set of observations that we explored with likelihood, in order to be able to compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the data\n",
    "flips = [\"H\",\"T\",\"T\",\"T\",\"H\"]\n",
    "# flips = flips*100  # Uncomment (and modify) this line to create a more informative dataset\n",
    "n = len(flips)        # Number (n) of binomial trials\n",
    "k = sum([1 for fl in flips if fl == \"H\"])  # Counting heads as successes (k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we're going to be sampling from the posterior distribution for *p*, the binomial parameter. Below we define three functions to calculate the likelihood, the prior, and the posterior. Since *p* only has meaning on the interval [0,1], the likelihood function will return 0 for any values outside that range. Similarly, we are using a uniform prior for [0,1] - this has a constant density of 1 in that range and 0, otherwise. Even though the function to calculate prior density only has a single line, defining such a function allows us to use prior() throughout the code and only change the one line in the function if we decide to change our prior specification. The unnormalized posterior density is calculated simply by multiplying the likelihood and the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining general function to calculate likelihoods if p<0 or p>1\n",
    "def like(successes,trials,prob,testingPrior=False):\n",
    "    if testingPrior:  # If True, this will always return 1. This can be useful if one wants\n",
    "        return 1      # to test the machinery by estimating a known distribution (the prior).\n",
    "    if prob < 0:\n",
    "        return 0\n",
    "    elif prob > 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return binom.pmf(successes,trials,prob)\n",
    "    \n",
    "# Defining function to calculate prior density - uniform [0,1]\n",
    "def prior(prob):\n",
    "    return uniform.pdf(prob)\n",
    "\n",
    "# Defining function to calculate the unnormalized posterior density\n",
    "def posterior(successes,trials,prob):\n",
    "    posterior = prior(prob) * like(successes,trials,prob)\n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the general idea of importance sampling. We don't know how to sample directly from some distribution we're interested in (we'll call it *p*) and/or we don't know how to calculate a relevant quantity (like the mean). We may not even be able to directly calculate probability densities of *p*. However, we *do* know how to sample from another distribution (say, *q*) that covers the same range of values as *p* and we can at least calculate an unnormalized density that is proportional to the pdf of *p*. If so, we can make *n* draws from the sampling distribution (*q*), then calculate weights (e.g., $w_1, w_2,\\ldots,w_n$) for these samples (e.g., $x_1,x_2,\\ldots,x_n$) to adjust for the fact that they were drawn from *q* and not *p*. These weights correspond to the ratio of the two probability densities (e.g., $\\frac{p(x_1)}{q(x_1)}$). To calculate values of interest for a random variable *X*, distributed according to *p* (i.e., $X \\sim p$), we can use the values we've drawn from *q* adjusted by their weights. For instance, $E[X]\\ =\\ x_1w_1+x_2w_2+\\ldots+x_nw_n\\ =\\ x_1\\frac{p(x_1)}{q(x_1)}+x_2\\frac{p(x_2)}{q(x_2)}+\\ldots+x_n\\frac{p(x_n)}{q(x_n)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To get a sense for how well importance sampling is working, we're going to run our\n",
    "# experiment several times. This list will hold the estimates of the means for all \n",
    "# replicates.\n",
    "estimates = []\n",
    "\n",
    "# The number of replicates we will run.\n",
    "numReps = 100\n",
    "\n",
    "# This value establishes the upper end of the uniform distribution from which we will sample\n",
    "# parameter values. Since only values between 0 and 1 can have likelihoods > 0, the more we\n",
    "# extend this value above 1, the greater the disparity between our sampling distribution and\n",
    "# distribution of interest. What happens as this gets bigger?\n",
    "uniScale = 1\n",
    "\n",
    "# Initializing our ad hoc progress bar\n",
    "print(\"Progress (each . = 10 replicates): \",end=\"\")\n",
    "\n",
    "# Iterating across our replicates 0,...,numReps-1\n",
    "for rep in range(numReps):\n",
    "    \n",
    "    # Incrementing our progress\n",
    "    if rep % 10 == 0:\n",
    "        print(\".\",end=\"\")\n",
    "    \n",
    "    # Draw values from uniform prior using the uniform class we imported from scipy\n",
    "    numValues = 100\n",
    "    p_vals = uniform.rvs(size=numValues,loc=0,scale=uniScale)\n",
    "    \n",
    "    # Calculate initial weights (not necessarily normalized)\n",
    "    weights = [(posterior(k,n,param)/uniform.pdf(param,loc=0,scale=uniScale)) for param in p_vals]\n",
    "    \n",
    "    # Normalize weights so average is 1\n",
    "    \"\"\"NOTE: This normalization isn't strictly necessary if both functions used to calculate \n",
    "           the initial weights are proper probability density functions. But even then, it \n",
    "           helps with rounding error.\"\"\"\n",
    "    weights = [w/np.mean(weights) for w in weights]\n",
    "        \n",
    "    # Calculating weighted average\n",
    "    count = 0\n",
    "    estMean = 0\n",
    "    while (count < (len(p_vals))):\n",
    "        estMean += p_vals[count]*weights[count]  # Multiplying each value by its weight\n",
    "        count += 1\n",
    "    estMean /= numValues\n",
    "    estimates.append(estMean)\n",
    "    \n",
    "# Printing out some useful summary information\n",
    "print()\n",
    "print(\"The last estimated mean (replicate %d): %f.\" % (numReps,estimates[numReps-1]))\n",
    "print(\"The mean of the estimated means across all %d replicates: %f.\" % (numReps,np.mean(estimates)))\n",
    "print(\"The standard deviation of the estimated means is: %f.\"% (np.std(estimates)))\n",
    "\n",
    "\n",
    "# Rerun this several times, varying the size of the dataset, the upper end of the sampling\n",
    "# distribution, and the number of values drawn (numValues) from the sampling distribution. \n",
    "# Pay attention to whether/how several things change:\n",
    "# - Is an error thrown?\n",
    "# - The estimated means\n",
    "# - The standard deviation of the estimated means\n",
    "# - The run time\n",
    "\n",
    "# Now try changing the testingPrior argument to True for the likelihood function.\n",
    "# What happens? Why might this be useful?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
